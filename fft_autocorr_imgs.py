import scipy.signal
import numpy as np
import matplotlib.pyplot as plt
from os import path, listdir
from PIL import Image
import pandas
from calculate_correlations import find_N_pairs, random_walk


###########3
# A script meant to play around with fft autocorrelation functions

def fft_autocorr_imgs(img_paths, method):


    acorr = []
    acorr_f = []
    # for i, img_path in enumerate(img_paths):
    #     img = np.array(Image.open(img_path).convert('L'))/255
    #     acorr.append( fft_autocorr(img )
    nn = 0
    r = range(nn, nn + 40)
    for i in r:
        img_path = img_paths[i]
        img = np.array(Image.open(img_path).convert('L'))/255
        # corr = correlation_distance(img, 1, 300, 1000)
        corr = fft_autocorr(img, method)

        if i == r[0]:
            shape = np.shape(corr)
            iu = np.triu_indices(n=shape[0], m=shape[1])
        elif not np.shape(img) == shape:
            shape = np.shape(corr)
            iu = np.triu_indices(n=shape[0], m=shape[1])


        acorr.append( corr[iu] )

    return acorr

def fft_autocorr(img, method, pad=False):
    # TODO: does the 'nm' normalization make sense? seems like it doesn't...


    img_mean = np.mean(np.mean(img))  # calculate mean
    img = img - img_mean

    if method == 'fft_windowed':
        # apply hanning window function
        h = np.outer( np.hanning(len(img)), np.hanning(len(img)) )
        img = img * h

    if pad:
        img = np.pad(img, len(img), 'constant')


    res = scipy.signal.fftconvolve(img, img[::-1, ::-1])
    norm = scipy.signal.convolve(img**2, np.ones(np.shape(img)))
    # nm = np.product(np.shape(img))


    return res / norm



def correlation_distance(img_in, dmin, dmax, N):
    # Calculate correlation function (correlation as a function of distance).

    # Summary: for each distance bin, find N pairs of points in the image with that distance. Calculate correlation
    # coefficient for the vectors generated by these pairs. Loop through all distance bins to get correlations as a
    # function of distance.

    # --- PARAMETERS ---
    # imgs_dir: directory of images
    # dmin, dmax: minimum and maximum distances
    # N: number of point pairs samples
    # ------------------
    img = img_in.flatten()
    corr_d = []
    res = np.sqrt(img.shape[0])  # img should be square...

    for d in np.arange(dmin, dmax):

        p, q = find_N_pairs(d, N, res)
        corr = np.corrcoef(img[p], img[q])[0, 1]

        corr_d.append(corr)


    return corr_d


def dist_vec(shape):
    n = shape[0]
    m = shape[1]
    iu = np.triu_indices(n=n, m=m, k=0)

    return np.sqrt((iu[0] - (n-1)/2)**2 + (iu[1] - (m-1)/2)**2)

def top_rec(n, m):
    n = int(n/2)
    iu = [(y, x) for x in range(m) for y in range(n)]
    return [np.array([iu[i][j] for i in range(len(iu))]) for j in range(2)]

def top_dist_vec(shape):
    n = shape[0]
    m = shape[1]
    iu = top_rec(n, m)

    return np.sqrt((iu[0] - (n - 1) / 2) ** 2 + (iu[1] - (m - 1) / 2) ** 2)


def theta_vec(shape):
    n = shape[0]
    m = shape[1]
    iu = np.triu_indices(n=n, m=m, k=0)


    thetau = np.arctan2((iu[0] - (n-1)/2), (iu[1] - (m-1)/2))

    return thetau

def norm_vec(shape):
    n = shape[0]
    m = shape[1]
    iu = np.triu_indices(n=n, m=m, k=0)

    # normsu = (n+1)/2 * (m+1)/2 / \
    #         (((n+1)/2 - np.abs((iu[0] - (n - 1) / 2)) ) *
    #          ((m+1)/2 - np.abs((iu[1] - (m - 1) / 2)) ))

    normsu = (n+1) * (m+1) / \
            (((n+1)/2 - np.abs((iu[0] - (n - 1) / 2)) ) *
             ((m+1)/2 - np.abs((iu[1] - (m - 1) / 2)) ) )

    return normsu

def norm_vec2(img, shape):
    n = shape[0]
    m = shape[1]
    N = int((n-1)/2)
    M = int((m-1)/2)

    iu = np.triu_indices(n=n, m=m, k=0)

    dn = (iu[0] - N)
    dm = (iu[1] - M)

    # Nsum_from = (( dn >= 0) * dn).astype('int')
    # Nsum_to = (( (N - dn <= N) * dn ) + ( N * (N - dn >= N) )).astype('int')
    # Msum_from = (( dm >= 0) * dm).astype('int')
    # Msum_to = (( (M - dm <= M) * dm ) + ( M * (M - dm >= M) )).astype('int')

    Nsum_from = np.maximum(dn, 0).astype('int')
    Nsum_to = np.minimum(N - dn, N).astype('int')

    Msum_from = np.maximum(dm, 0).astype('int')
    Msum_to = np.minimum(M - dm, M).astype('int')

    # normsu =  [np.mean(img[Nsum_from[i]:Nsum_to[i], Msum_from[i]:Msum_to[i]]**2)
    #            for i in range(len(iu[0]))]

    normsu =  [np.mean(img[Nsum_from[i]:N, Msum_from[i]:M]**2)
               for i in range(len(iu[0]))]

    return normsu


def coord_mat(shape):
    x_dim = shape[0]
    y_dim = shape[1]

    coord = np.array([[(x, y) for x in range(x_dim)] for y in range(y_dim)])


    return coord

def dist_mat(shape):
    n = shape[0]
    m = shape[1]
    dist = np.zeros((n, m))

    i = np.triu_indices(n=n, m=m, k=0)
    dist[i] = np.sqrt((i[0] - (n-1)/2)**2 + (i[1] - (m-1)/2)**2)

    i = np.tril_indices(n=n, m=m, k=1)
    dist[i] = np.sqrt((i[0] - (n - 1) / 2) ** 2 + (i[1] - (m - 1) / 2) ** 2)

    return dist


def load_image_directories(img_folder=None):
    # make a list of all imgs in image directory

    dir = 'save/img_architectures'
    imgs_folder = path.join(dir, img_folder)
    imgs_paths = [path.join(imgs_folder, f) for f in listdir(imgs_folder) if f.endswith('.png')]

    return imgs_paths


if __name__ == '__main__':

    img_folder = 'big'
    method = 'fft'
    img_paths = load_image_directories(img_folder)

    acorr = fft_autocorr_imgs(img_paths, method)

    ## PLOT CORR FUNCTION FROM FFT AUTOCORRS ##
    # for i in range(0, 120, 40):
    #     shape = np.shape(acorr[0])  # get shape from first img
    #     distances = dist_vec((1023, 1023))
    #     acorr_f_plot = np.mean(acorr[i:i+40], axis=0)
    #     plt.scatter(distances, acorr_f_plot, s=0.01, alpha=0.05)
    # plt.show()
    ###########################################


    ## PLOT FFT AUTOCORRS ##
    # corr_plot = np.mean(acorr, axis=0)
    # plt.imshow(corr_plot)
    # plt.contour(corr_plot, 20, colors='w', alpha=0.1)
    #
    # plt.show()
    #########################

    # distances = dist_vec((1023, 1023))
    # plt.figure()
    #
    # for i in range(0, 480, 40):
    #     start = i
    #     end = start + 40
    #
    #     acorr_m = np.mean(np.stack(acorr[start:end]), axis=0)
    #     acorr_q25 = np.quantile(np.stack(acorr[start:end]), 0.25, axis=0)
    #     acorr_q75 = np.quantile(np.stack(acorr[start:end]), 0.75, axis=0)
    #
    #     corr_means, bin_edges, _ = scipy.stats.binned_statistic(distances, acorr_m, statistic='mean', bins=300)
    #     corr_q25, _, _ = scipy.stats.binned_statistic(distances, acorr_q25, statistic='mean', bins=300)
    #     corr_q75, _, _ = scipy.stats.binned_statistic(distances, acorr_q75, statistic='mean', bins=300)
    #
    #     d = (bin_edges[:-1] + bin_edges[1:]) / 2
    #
    #     plt.plot(d, corr_means)
    #     plt.fill_between(d, corr_q75, corr_q25, alpha=0.05)
    #
    # plt.show()


# img = np.array(Image.open(img_path).convert('L'))/255
# m = np.mean(np.mean(img))
# img = img - m
# corr = fft_autocorr(img)
#
# img = np.pad(img, len(img), 'constant')
# corr_p = fft_autocorr(img)
#
# img = np.array(Image.open(img_path).convert('L'))/255
# img = img - m
# img = np.pad(img, int(len(img)/2), 'constant')
# corr_p2 = fft_autocorr(img)
#
# ### WINDOWING ###
#
# img = np.array(Image.open(img_path).convert('L'))/255
# m = np.mean(np.mean(img))
# h = np.outer(np.hanning(len(img)), np.hanning(len(img)))
# img = h * (img - m)
# corr_w = fft_autocorr(img)
#
# img = np.pad(img, len(img), 'constant')
# corr_wp = fft_autocorr(img)
#
# img = np.array(Image.open(img_path).convert('L'))/255
# img = h * (img - m)
# img = np.pad(img, int(len(img)/2), 'constant')
# corr_wp2 = fft_autocorr(img)
#
# fig, ax = plt.subplots(2, 3)
#
#
# im1 = ax[0, 0].imshow(corr)
# ax[0,0].set_title('No padding')
# im2 = ax[0, 1].imshow(corr_p[1023:-1023, 1023:-1023])
# ax[0,1].set_title('Full padding')
# im3 = ax[0, 2].imshow(corr_p2[512:-512, 512:-512])
# ax[0,2].set_title('Half padding')
#
# im4 = ax[1, 0].imshow(corr_w)
# im5 = ax[1, 1].imshow(corr_wp[1023:-1023, 1023:-1023])
# ax[1, 1].set_title('With 2D Hanning Window')
# im6 = ax[1, 2].imshow(corr_wp2[512:-512, 512:-512])
#
# fig.colorbar(im1, shrink=0.4, ax=ax[0, 0])
# fig.colorbar(im2, shrink=0.4, ax=ax[0, 1])
# fig.colorbar(im3, shrink=0.4, ax=ax[0, 2])
#
# fig.colorbar(im4, shrink=0.4, ax=ax[1, 0])
# fig.colorbar(im5, shrink=0.4, ax=ax[1, 1])
# fig.colorbar(im6, shrink=0.4, ax=ax[1, 2])
#
# fig.suptitle('FFT Autocorrelation')
#
#
# ### DIFFERENCES ###
# fig, ax = plt.subplots(2, 3)
#
# diff_p = corr - corr_p[1024:-1024, 1024:-1024]
# diff_p2 = corr - corr_p2[512:-512, 512:-512]
# diff_p_p2 = diff_p - diff_p2
#
# diff_wp = corr_w - corr_wp[1024:-1024, 1024:-1024]
# diff_wp2 = corr_w - corr_wp2[512:-512, 512:-512]
# diff_wp_wp2 = diff_wp - diff_wp2
#
# im1 = ax[0, 0].imshow(diff_p)
# ax[0,0].set_title('Full padding')
# im2 = ax[0, 1].imshow(diff_p2)
# ax[0,1].set_title('Half padding')
# im3 = ax[0, 2].imshow(diff_p_p2)
# ax[0,2].set_title('Full - Half')
#
#
# im4 = ax[1, 0].imshow(diff_wp)
# im5 = ax[1, 1].imshow(diff_wp2)
# ax[1, 1].set_title('With 2D Hanning Window')
# im6 = ax[1, 2].imshow(diff_wp_wp2)
#
# fig.colorbar(im1, shrink=0.4, ax=ax[0, 0])
# fig.colorbar(im2, shrink=0.4, ax=ax[0, 1])
# fig.colorbar(im3, shrink=0.4, ax=ax[0, 2])
#
# fig.colorbar(im4, shrink=0.4, ax=ax[1, 0])
# fig.colorbar(im5, shrink=0.4, ax=ax[1, 1])
# fig.colorbar(im6, shrink=0.4, ax=ax[1, 2])
#
#
# fig.suptitle('FFT Autocorrelation Differences')
#
# ### DIFFERENCE BETWEEN NO WINDOW AND WINDOW ###
#
# fig, ax = plt.subplots(1)
#
# diff_c_wc = corr - corr_w
#
# im1 = ax.imshow(diff_c_wc)
#
# fig.colorbar(im1, shrink=0.4, ax=ax)
# fig.suptitle('Correlation difference between non-windowed and windowed')


#############################3
# xv, yv = np.meshgrid(range(300), range(100))
# img = np.sin(10 * np.pi * xv / len(xv))
# corr = fft_autocorr(img, 'fft')
# corr2 = fft_autocorr(img, 'fft_windowed')
# h = np.bartlett(len(img))
# h_b = np.outer(h, h)
# corr3 = fft_autocorr(img*h_b, 'fft')
# fig, ax = plt.subplots(2, 3)
# ax[0,0].imshow(corr)
# ax[0,1].imshow(corr2)
# ax[0,2].imshow(corr3)
# ax[1,0].plot(corr[1000, 1000:])
# ax[1,1].plot(corr2[1000, 1000:])
# ax[1,2].plot(corr3[1000, 1000:])
# x = np.arange(99,1000,100)
# for i in zip([1, 1, 1], [0, 1, 2]):
#     for xi in x:
#         ax[i].axvline(xi, c='r', linestyle='--')


# import numpy as np
# import matplotlib.pyplot as plt
# from scipy.spatial.distance import cdist
#
# dim = 500
# xdim = dim
# ydim = dim
# coord = np.array([
#     [(x,y) for x in range(xdim)] for y in range(ydim)
# ]).reshape(xdim*ydim, -1)
# iu = np.tril_indices(len(coord), 1)
# distances = cdist(coord, coord)[iu]
#
# distances = np.zeros( int((len(coord)**2 - len(coord) ) / 2) )
# ii = 0
# for i in range(len(coord) - 1):
#     for j in range(i + 1, len(coord)):
#         distances[ii] = np.sqrt((coord[i][0] - coord[j][0]) ** 2 + (coord[i][1] - coord[j][1]) ** 2)
#         ii += 1
#
# plt.hist(distances, 100)
#################################################
#################################################
############ THETA COLOUR CODING ###############
# distances = dist_vec((1023,1023))
# thetas = -theta_vec((1023,1023)) * 180 / np.pi
#
# corr = fft_autocorr(img, 'fft')
#
# iu = np.triu_indices(1023)

#################################################
# plt.figure(figsize=(15, 8))
# plt.scatter(distances, corr[iu], s=0.1, alpha=0.1, c=thetas, cmap='plasma')
# plt.title('Correlation function of CPPN Image')
# plt.xlabel('Distance')
# plt.ylabel('Correlation')

# cbar = plt.colorbar()
#
# cbar.set_alpha(1)
# cbar.draw_all()
# cbar.ax.get_yaxis().set_ticks([135, 90, 45, 0, -45])
# cbar.set_label('Theta')
#################################################

#################################################
# plt.figure(figsize=(15, 8))
# plt.scatter(thetas, corr[iu], s=0.1, alpha=0.1, c=distances, cmap='plasma')
# plt.title('Correlation function of CPPN Image')
# plt.xlabel('Theta')
# plt.ylabel('Correlation')
#
# cbar = plt.colorbar()
#
# cbar.set_alpha(1)
# cbar.draw_all()
# cbar.set_label('Distances')
#################################################

# imgs_folder = '/home/skhajehabdollahi/Documents/cppn-tensorflow-master/art_imgs/train'
# imgs_paths = [path.join(imgs_folder, f) for f in listdir(imgs_folder) if f.endswith('.jpg')]
#
# corrs = []
# numImgs = 10
# for impath in imgs_paths[0:numImgs]:
#     img = np.array(Image.open(impath).convert('L')) / 255
#     corrs.append(fft_autocorr(img, 'fft'))
#
# i = 3
# img = np.array(Image.open(imgs_paths[i])) / 255
# shape = np.shape(corrs[i])
# distances = top_dist_vec(shape)
# iu = top_rec(n=shape[0], m=shape[1])
#
# ax1 = plt.subplot(221)
# ax2 = plt.subplot(222)
# ax3 = plt.subplot(212)
#
# ax1.imshow(img)
# ax1.set_title('Original image')
#
# ax2.imshow(corrs[i])
# ax2.set_title('Auto-correlation image')
#
# alpha = np.tanh(0.05 + 1 / np.size(corrs[i][iu]) )
# ax3.scatter(distances, corrs[i][iu], s=0.03, alpha=alpha)
# ax3.set_title('Correlation function')
# ax3.set_xlabel('Distance')
# ax3.set_ylabel('Correlation Coefficient')